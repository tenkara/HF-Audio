{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install git+https://github.com/huggingface/transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python.exe -m pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keyword Spotting (KWS) is the task of identifying a keyword in a spoken utterance. The set of keywords forms the set of predicted class labels. Hence, to use a pre-trained keyword spotting model, you should ensure that your keywords match those that the model was pre-trained on. Below, we’ll introduce **two datasets and models** for keyword spotting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Minds-14**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Raj\\repos\\HF-Audio\\4-music-genre-classifier\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Downloading builder script: 100%|██████████| 5.95k/5.95k [00:00<00:00, 5.95MB/s]\n",
      "Downloading readme: 100%|██████████| 5.29k/5.29k [00:00<?, ?B/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset minds14/en-AU to C:/Users/Raj/.cache/huggingface/datasets/PolyAI___minds14/en-AU/1.0.0/65c7e0f3be79e18a6ffaf879a083daf706312d421ac90d25718459cbf3c42696...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 471M/471M [00:40<00:00, 11.6MB/s] \n",
      "                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset minds14 downloaded and prepared to C:/Users/Raj/.cache/huggingface/datasets/PolyAI___minds14/en-AU/1.0.0/65c7e0f3be79e18a6ffaf879a083daf706312d421ac90d25718459cbf3c42696. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "# Load the minds-14 dataset\n",
    "from datasets import load_dataset\n",
    "# import minds-14k dataset\n",
    "minds = load_dataset(\"PolyAI/minds14\", name=\"en-AU\", split=\"train\", download_mode=\"force_redownload\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the checkpoint \"anton-l/xtreme_s_xlsr_300m_minds14\", which is an XLS-R model fine-tuned on MINDS-14 for approximately 50 epochs. It achieves 90% accuracy over all languages from MINDS-14 on the evaluation set.\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\n",
    "    \"audio-classification\",\n",
    "    model=\"anton-l/xtreme_s_xlsr_300m_minds14\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting soundfile\n",
      "  Using cached soundfile-0.12.1-py2.py3-none-win_amd64.whl (1.0 MB)\n",
      "Collecting librosa\n",
      "  Using cached librosa-0.10.0.post2-py3-none-any.whl (253 kB)\n",
      "Collecting cffi>=1.0 (from soundfile)\n",
      "  Using cached cffi-1.15.1-cp311-cp311-win_amd64.whl (179 kB)\n",
      "Collecting audioread>=2.1.9 (from librosa)\n",
      "  Using cached audioread-3.0.0-py3-none-any.whl\n",
      "Requirement already satisfied: numpy!=1.22.0,!=1.22.1,!=1.22.2,>=1.20.3 in c:\\users\\raj\\repos\\hf-audio\\4-music-genre-classifier\\.venv\\lib\\site-packages (from librosa) (1.24.4)\n",
      "Collecting scipy>=1.2.0 (from librosa)\n",
      "  Using cached scipy-1.11.1-cp311-cp311-win_amd64.whl (44.0 MB)\n",
      "Collecting scikit-learn>=0.20.0 (from librosa)\n",
      "  Using cached scikit_learn-1.3.0-cp311-cp311-win_amd64.whl (9.2 MB)\n",
      "Collecting joblib>=0.14 (from librosa)\n",
      "  Using cached joblib-1.3.1-py3-none-any.whl (301 kB)\n",
      "Requirement already satisfied: decorator>=4.3.0 in c:\\users\\raj\\repos\\hf-audio\\4-music-genre-classifier\\.venv\\lib\\site-packages (from librosa) (5.1.1)\n",
      "Collecting numba>=0.51.0 (from librosa)\n",
      "  Using cached numba-0.57.1-cp311-cp311-win_amd64.whl (2.6 MB)\n",
      "Collecting pooch<1.7,>=1.0 (from librosa)\n",
      "  Using cached pooch-1.6.0-py3-none-any.whl (56 kB)\n",
      "Collecting soxr>=0.3.2 (from librosa)\n",
      "  Using cached soxr-0.3.5-cp311-cp311-win_amd64.whl (184 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.1.1 in c:\\users\\raj\\repos\\hf-audio\\4-music-genre-classifier\\.venv\\lib\\site-packages (from librosa) (4.7.1)\n",
      "Collecting lazy-loader>=0.1 (from librosa)\n",
      "  Using cached lazy_loader-0.3-py3-none-any.whl (9.1 kB)\n",
      "Requirement already satisfied: msgpack>=1.0 in c:\\users\\raj\\repos\\hf-audio\\4-music-genre-classifier\\.venv\\lib\\site-packages (from librosa) (1.0.5)\n",
      "Requirement already satisfied: pycparser in c:\\users\\raj\\repos\\hf-audio\\4-music-genre-classifier\\.venv\\lib\\site-packages (from cffi>=1.0->soundfile) (2.21)\n",
      "Collecting llvmlite<0.41,>=0.40.0dev0 (from numba>=0.51.0->librosa)\n",
      "  Using cached llvmlite-0.40.1-cp311-cp311-win_amd64.whl (27.7 MB)\n",
      "Requirement already satisfied: appdirs>=1.3.0 in c:\\users\\raj\\repos\\hf-audio\\4-music-genre-classifier\\.venv\\lib\\site-packages (from pooch<1.7,>=1.0->librosa) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\raj\\repos\\hf-audio\\4-music-genre-classifier\\.venv\\lib\\site-packages (from pooch<1.7,>=1.0->librosa) (23.1)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\users\\raj\\repos\\hf-audio\\4-music-genre-classifier\\.venv\\lib\\site-packages (from pooch<1.7,>=1.0->librosa) (2.31.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\raj\\repos\\hf-audio\\4-music-genre-classifier\\.venv\\lib\\site-packages (from scikit-learn>=0.20.0->librosa) (3.1.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\raj\\repos\\hf-audio\\4-music-genre-classifier\\.venv\\lib\\site-packages (from requests>=2.19.0->pooch<1.7,>=1.0->librosa) (3.2.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\raj\\repos\\hf-audio\\4-music-genre-classifier\\.venv\\lib\\site-packages (from requests>=2.19.0->pooch<1.7,>=1.0->librosa) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\raj\\repos\\hf-audio\\4-music-genre-classifier\\.venv\\lib\\site-packages (from requests>=2.19.0->pooch<1.7,>=1.0->librosa) (2.0.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\raj\\repos\\hf-audio\\4-music-genre-classifier\\.venv\\lib\\site-packages (from requests>=2.19.0->pooch<1.7,>=1.0->librosa) (2023.5.7)\n",
      "Installing collected packages: soxr, scipy, llvmlite, lazy-loader, joblib, cffi, audioread, soundfile, scikit-learn, pooch, numba, librosa\n",
      "Successfully installed audioread-3.0.0 cffi-1.15.1 joblib-1.3.1 lazy-loader-0.3 librosa-0.10.0.post2 llvmlite-0.40.1 numba-0.57.1 pooch-1.6.0 scikit-learn-1.3.0 scipy-1.11.1 soundfile-0.12.1 soxr-0.3.5\n"
     ]
    }
   ],
   "source": [
    "! pip install soundfile librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.9623647332191467, 'label': 'pay_bill'},\n",
       " {'score': 0.028678378090262413, 'label': 'freeze'},\n",
       " {'score': 0.0034296174999326468, 'label': 'card_issues'},\n",
       " {'score': 0.002060487400740385, 'label': 'abroad'},\n",
       " {'score': 0.0008625671616755426, 'label': 'high_value_payment'}]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pass a sample to the classification pipeline to make a prediction\n",
    "classifier(minds[0][\"path\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Speech Commands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the speech commands dataset\n",
    "from datasets import load_dataset\n",
    "speech_commands = load_dataset(\n",
    "    \"speech_commands\", \"v0.02\", split=\"validation\", streaming=True\n",
    ")\n",
    "sample = next(iter(speech_commands))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'file': 'backward/0d82fd99_nohash_2.wav',\n",
       " 'audio': {'path': 'backward/0d82fd99_nohash_2.wav',\n",
       "  'array': array([-9.15527344e-05,  6.10351562e-05,  6.10351562e-05, ...,\n",
       "          2.68859863e-02,  1.46179199e-02,  1.67846680e-03]),\n",
       "  'sampling_rate': 16000},\n",
       " 'label': 30,\n",
       " 'is_unknown': True,\n",
       " 'speaker_id': '0d82fd99',\n",
       " 'utterance_id': 2}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-9.15527344e-05,  6.10351562e-05,  6.10351562e-05, ...,\n",
       "        2.68859863e-02,  1.46179199e-02,  1.67846680e-03])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample[\"audio\"][\"array\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'score': 0.9999892711639404, 'label': 'backward'},\n",
       " {'score': 1.7504938796264469e-06, 'label': 'happy'},\n",
       " {'score': 6.703040185129794e-07, 'label': 'follow'},\n",
       " {'score': 5.805884484288981e-07, 'label': 'stop'},\n",
       " {'score': 5.614535893982975e-07, 'label': 'up'}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load a speech commands model\n",
    "from transformers import pipeline\n",
    "classifier = pipeline(\n",
    "    \"audio-classification\", model=\"MIT/ast-finetuned-speech-commands-v2\"\n",
    ")\n",
    "classifier(sample[\"audio\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'array'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m sample[\u001b[39m\"\u001b[39;49m\u001b[39maudio\u001b[39;49m\u001b[39m\"\u001b[39;49m][\u001b[39m\"\u001b[39;49m\u001b[39marray\u001b[39;49m\u001b[39m\"\u001b[39;49m]\n",
      "\u001b[1;31mKeyError\u001b[0m: 'array'"
     ]
    }
   ],
   "source": [
    "sample[\"audio\"][\"array\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'array'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# we can listen to the audio sample and verify that the prediction 'backward' is correct\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mIPython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdisplay\u001b[39;00m \u001b[39mimport\u001b[39;00m Audio\n\u001b[1;32m----> 4\u001b[0m Audio(sample[\u001b[39m\"\u001b[39;49m\u001b[39maudio\u001b[39;49m\u001b[39m\"\u001b[39;49m][\u001b[39m\"\u001b[39;49m\u001b[39marray\u001b[39;49m\u001b[39m\"\u001b[39;49m], rate\u001b[39m=\u001b[39msample[\u001b[39m\"\u001b[39m\u001b[39maudio\u001b[39m\u001b[39m\"\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39msampling_rate\u001b[39m\u001b[39m\"\u001b[39m])\n",
      "\u001b[1;31mKeyError\u001b[0m: 'array'"
     ]
    }
   ],
   "source": [
    "# we can listen to the audio sample and verify that the prediction 'backward' is correct\n",
    "from IPython.display import Audio\n",
    "\n",
    "Audio(sample[\"audio\"][\"array\"], rate=sample[\"audio\"][\"sampling_rate\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
