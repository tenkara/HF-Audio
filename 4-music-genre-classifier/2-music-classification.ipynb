{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train our model, weâ€™ll use the GTZAN dataset, which is a popular dataset of 1,000 songs for music genre classification. Each song is a 30-second clip from one of 10 genres of music, spanning disco to metal. We can get the audio files and their corresponding labels from the Hugging Face Hub with the load_dataset() function from ðŸ¤— Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset gtzan (C:/Users/Raj/.cache/huggingface/datasets/marsyas___gtzan/all/0.0.0/a41ab02ead4abe36cdf531ccdfde57aaee72b3bda7319f3bb61eb8d8b9caebcf)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba0f878153d442609adb7bff802727a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['file', 'audio', 'genre'],\n",
       "        num_rows: 999\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "gtzan = load_dataset(\"marsyas/gtzan\", \"all\")\n",
    "gtzan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GTZAN doesnâ€™t provide a predefined validation set, so weâ€™ll have to create one ourselves. The dataset is balanced across genres, so we can use the train_test_split() method to quickly create a 90/10 split as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['file', 'audio', 'genre'],\n",
       "        num_rows: 899\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['file', 'audio', 'genre'],\n",
       "        num_rows: 100\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gtzan = gtzan[\"train\"].train_test_split(seed=42, shuffle=True, test_size=0.1)\n",
    "gtzan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'file': 'C:\\\\Users\\\\Raj\\\\.cache\\\\huggingface\\\\datasets\\\\downloads\\\\extracted\\\\61c891d8d3b801632ad634a2a14e2e2c1a2455ea91af46e11e266c56c97fb685\\\\genres\\\\pop\\\\pop.00098.wav',\n",
       " 'audio': {'path': 'C:\\\\Users\\\\Raj\\\\.cache\\\\huggingface\\\\datasets\\\\downloads\\\\extracted\\\\61c891d8d3b801632ad634a2a14e2e2c1a2455ea91af46e11e266c56c97fb685\\\\genres\\\\pop\\\\pop.00098.wav',\n",
       "  'array': array([ 0.10720825,  0.16122437,  0.28585815, ..., -0.22924805,\n",
       "         -0.20629883, -0.11334229]),\n",
       "  'sampling_rate': 22050},\n",
       " 'genre': 7}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's take a look at one of the audio files\n",
    "gtzan[\"train\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also see the genre is represented as an integer, or class label, which is the format the model will make itâ€™s predictions in. Letâ€™s use the int2str() method of the genre feature to map these integers to human-readable names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'pop'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id2label_fn = gtzan[\"train\"].features[\"genre\"].int2str\n",
    "id2label_fn(gtzan[\"train\"][0][\"genre\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This label looks correct, since it matches the filename of the audio file. Letâ€™s now listen to a few more examples by using Gradio to create a simple interface with the Blocks API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Raj\\repos\\HF-Audio\\4-music-genre-classifier\\.venv\\Lib\\site-packages\\gradio\\processing_utils.py:183: UserWarning: Trying to convert audio automatically from float64 to 16-bit int format.\n",
      "  warnings.warn(warning.format(data.dtype))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keyboard interruption in main thread... closing server.\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "def generate_audio():\n",
    "    example = gtzan[\"train\"].shuffle()[0]\n",
    "    audio = example[\"audio\"]\n",
    "    return (\n",
    "        audio[\"sampling_rate\"],\n",
    "        audio[\"array\"],\n",
    "    ), id2label_fn(example[\"genre\"])\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    with gr.Column():\n",
    "        for _ in range(4):\n",
    "            audio, label = generate_audio()\n",
    "            output = gr.Audio(audio, label=label)\n",
    "\n",
    "demo.launch(debug=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Picking a pretrained model for audio classification**\n",
    "\n",
    "Although models like Wav2Vec2 and HuBERT are very popular, weâ€™ll use a model called DistilHuBERT. This is a much smaller (or distilled) version of the HuBERT model, which trains around 73% faster, yet preserves most of the performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing the data\n",
    "\n",
    "Similar to tokenization in NLP, audio and speech models require the input to be encoded in a format that the model can process. In ðŸ¤— Transformers, the conversion from audio to the input format is handled by the feature extractor of the model. Similar to tokenizers, ðŸ¤— Transformers provides a convenient AutoFeatureExtractor class that can automatically select the correct feature extractor for a given model. To see how we can process our audio files, letâ€™s begin by instantiating the feature extractor for DistilHuBERT from the pre-trained checkpoint:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5c8cced285d4b4eafb214e7b63b3e71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (â€¦)rocessor_config.json:   0%|          | 0.00/214 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoFeatureExtractor\n",
    "\n",
    "model_id = \"ntu-spml/distilhubert\"\n",
    "feature_extractor = AutoFeatureExtractor.from_pretrained(\n",
    "    model_id, do_normalize=True, return_attention_mask=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16000"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampling_rate = feature_extractor.sampling_rate\n",
    "sampling_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the sampling rate of the model and the dataset are different, weâ€™ll have to resample the audio file to 16,000 Hz before passing it to the feature extractor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resample the dataset using the cast_column() method and Audio feature from Hf datasets\n",
    "from datasets import Audio\n",
    "gtzan = gtzan.cast_column(\"audio\", Audio(sampling_rate=sampling_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'file': 'C:\\\\Users\\\\Raj\\\\.cache\\\\huggingface\\\\datasets\\\\downloads\\\\extracted\\\\61c891d8d3b801632ad634a2a14e2e2c1a2455ea91af46e11e266c56c97fb685\\\\genres\\\\pop\\\\pop.00098.wav',\n",
       " 'audio': {'path': 'C:\\\\Users\\\\Raj\\\\.cache\\\\huggingface\\\\datasets\\\\downloads\\\\extracted\\\\61c891d8d3b801632ad634a2a14e2e2c1a2455ea91af46e11e266c56c97fb685\\\\genres\\\\pop\\\\pop.00098.wav',\n",
       "  'array': array([ 0.08735093,  0.20183387,  0.47908676, ..., -0.1874318 ,\n",
       "         -0.23294398, -0.13517427]),\n",
       "  'sampling_rate': 16000},\n",
       " 'genre': 7}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check out the first sample of the train-split of our dataset to verify it is indeded at 16,000Hz\n",
    "gtzan[\"train\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We normalize the audio data by feature scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: 0.000185, Variance: 0.0493\n"
     ]
    }
   ],
   "source": [
    "# First let's compute the mean and variance of our raw audio data\n",
    "import numpy as np\n",
    "sample = gtzan[\"train\"][0][\"audio\"]\n",
    "print(f\"Mean: {np.mean(sample['array']):.3}, Variance: {np.var(sample['array']):.3}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
