{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train our model, weâ€™ll use the GTZAN dataset, which is a popular dataset of 1,000 songs for music genre classification. Each song is a 30-second clip from one of 10 genres of music, spanning disco to metal. We can get the audio files and their corresponding labels from the Hugging Face Hub with the load_dataset() function from ðŸ¤— Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset gtzan (/home/raj/.cache/huggingface/datasets/marsyas___gtzan/all/0.0.0/8bd0e23c2d9b2be30d36bc6834319772dff22a3bd28527996612386cef003910)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce28a89c6e8341c1ba1a79872ef8dd04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['file', 'audio', 'genre'],\n",
       "        num_rows: 999\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "gtzan = load_dataset(\"marsyas/gtzan\", \"all\")\n",
    "gtzan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GTZAN doesnâ€™t provide a predefined validation set, so weâ€™ll have to create one ourselves. The dataset is balanced across genres, so we can use the train_test_split() method to quickly create a 90/10 split as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached split indices for dataset at /home/raj/.cache/huggingface/datasets/marsyas___gtzan/all/0.0.0/8bd0e23c2d9b2be30d36bc6834319772dff22a3bd28527996612386cef003910/cache-52d2398c8e4ac745.arrow and /home/raj/.cache/huggingface/datasets/marsyas___gtzan/all/0.0.0/8bd0e23c2d9b2be30d36bc6834319772dff22a3bd28527996612386cef003910/cache-3bcc56e346e4d81c.arrow\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['file', 'audio', 'genre'],\n",
       "        num_rows: 899\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['file', 'audio', 'genre'],\n",
       "        num_rows: 100\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gtzan = gtzan[\"train\"].train_test_split(seed=42, shuffle=True, test_size=0.1)\n",
    "gtzan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'file': '/home/raj/.cache/huggingface/datasets/downloads/extracted/fa0c0173870969fd11c975895603e8608b7325bb180f3aeb805320cfa0922824/genres/pop/pop.00098.wav',\n",
       " 'audio': {'path': '/home/raj/.cache/huggingface/datasets/downloads/extracted/fa0c0173870969fd11c975895603e8608b7325bb180f3aeb805320cfa0922824/genres/pop/pop.00098.wav',\n",
       "  'array': array([ 0.10720825,  0.16122437,  0.28585815, ..., -0.22924805,\n",
       "         -0.20629883, -0.11334229]),\n",
       "  'sampling_rate': 22050},\n",
       " 'genre': 7}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's take a look at one of the audio files\n",
    "gtzan[\"train\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also see the genre is represented as an integer, or class label, which is the format the model will make itâ€™s predictions in. Letâ€™s use the int2str() method of the genre feature to map these integers to human-readable names:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'pop'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id2label_fn = gtzan[\"train\"].features[\"genre\"].int2str\n",
    "id2label_fn(gtzan[\"train\"][0][\"genre\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This label looks correct, since it matches the filename of the audio file. Letâ€™s now listen to a few more examples by using Gradio to create a simple interface with the Blocks API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import gradio as gr\n",
    "# def generate_audio():\n",
    "#     example = gtzan[\"train\"].shuffle()[0]\n",
    "#     audio = example[\"audio\"]\n",
    "#     return (\n",
    "#         audio[\"sampling_rate\"],\n",
    "#         audio[\"array\"],\n",
    "#     ), id2label_fn(example[\"genre\"])\n",
    "\n",
    "# with gr.Blocks() as demo:\n",
    "#     with gr.Column():\n",
    "#         for _ in range(4):\n",
    "#             audio, label = generate_audio()\n",
    "#             output = gr.Audio(audio, label=label)\n",
    "\n",
    "# demo.launch(debug=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Picking a pretrained model for audio classification**\n",
    "\n",
    "Although models like Wav2Vec2 and HuBERT are very popular, weâ€™ll use a model called DistilHuBERT. This is a much smaller (or distilled) version of the HuBERT model, which trains around 73% faster, yet preserves most of the performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocessing the data\n",
    "\n",
    "Similar to tokenization in NLP, audio and speech models require the input to be encoded in a format that the model can process. In ðŸ¤— Transformers, the conversion from audio to the input format is handled by the feature extractor of the model. Similar to tokenizers, ðŸ¤— Transformers provides a convenient AutoFeatureExtractor class that can automatically select the correct feature extractor for a given model. To see how we can process our audio files, letâ€™s begin by instantiating the feature extractor for DistilHuBERT from the pre-trained checkpoint:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoFeatureExtractor\n",
    "\n",
    "model_id = \"ntu-spml/distilhubert\"\n",
    "feature_extractor = AutoFeatureExtractor.from_pretrained(\n",
    "    model_id, do_normalize=True, return_attention_mask=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16000"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sampling_rate = feature_extractor.sampling_rate\n",
    "sampling_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the sampling rate of the model and the dataset are different, weâ€™ll have to resample the audio file to 16,000 Hz before passing it to the feature extractor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# resample the dataset using the cast_column() method and Audio feature from Hf datasets\n",
    "from datasets import Audio\n",
    "gtzan = gtzan.cast_column(\"audio\", Audio(sampling_rate=sampling_rate))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'file': '/home/raj/.cache/huggingface/datasets/downloads/extracted/fa0c0173870969fd11c975895603e8608b7325bb180f3aeb805320cfa0922824/genres/pop/pop.00098.wav',\n",
       " 'audio': {'path': '/home/raj/.cache/huggingface/datasets/downloads/extracted/fa0c0173870969fd11c975895603e8608b7325bb180f3aeb805320cfa0922824/genres/pop/pop.00098.wav',\n",
       "  'array': array([ 0.0873509 ,  0.20183384,  0.4790867 , ..., -0.18743178,\n",
       "         -0.23294401, -0.13517427]),\n",
       "  'sampling_rate': 16000},\n",
       " 'genre': 7}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check out the first sample of the train-split of our dataset to verify it is indeded at 16,000Hz\n",
    "gtzan[\"train\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We normalize the audio data by feature scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: 0.000185, Variance: 0.0493\n"
     ]
    }
   ],
   "source": [
    "# First let's compute the mean and variance of our raw audio data\n",
    "import numpy as np\n",
    "sample = gtzan[\"train\"][0][\"audio\"]\n",
    "print(f\"Mean: {np.mean(sample['array']):.3}, Variance: {np.var(sample['array']):.3}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the mean is close to zero already, but the variance is closer to 0.05. If the variance for the sample was larger, it could cause our model problems, since the dynamic range of the audio data would be very small and thus difficult to separate. Letâ€™s apply the feature extractor and see what the outputs look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inputs keys: dict_keys(['input_values', 'attention_mask'])\n",
      "Mean: -7.45e-09, Variance: 1.0\n"
     ]
    }
   ],
   "source": [
    "inputs = feature_extractor(\n",
    "    sample[\"array\"], sampling_rate=sample[\"sampling_rate\"])\n",
    "print(f\"inputs keys: {inputs.keys()}\")\n",
    "print(f\"Mean: {np.mean(inputs['input_values']):.3}, Variance: {np.var(inputs['input_values']):.3}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the mean value is now very much closer to zero, and the variance bang-on one! This is exactly the form we want our audio samples in prior to feeding them to the HuBERT model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a preprocess function that will truncate longer clips to 30 seconds\n",
    "max_duration = 30.0\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    audio_arrays = [x[\"array\"] for x in examples[\"audio\"]]\n",
    "    inputs = feature_extractor(\n",
    "        audio_arrays, \n",
    "        sampling_rate=feature_extractor.sampling_rate,\n",
    "        max_length=int(feature_extractor.sampling_rate * max_duration),\n",
    "        truncation=True,\n",
    "        return_attention_mask=True,\n",
    "        padding=True,\n",
    "    )\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this function defined, we can now apply it to the dataset using the map() method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/raj/.cache/huggingface/datasets/marsyas___gtzan/all/0.0.0/8bd0e23c2d9b2be30d36bc6834319772dff22a3bd28527996612386cef003910/cache-fb0bf2497cafee86.arrow\n",
      "Loading cached processed dataset at /home/raj/.cache/huggingface/datasets/marsyas___gtzan/all/0.0.0/8bd0e23c2d9b2be30d36bc6834319772dff22a3bd28527996612386cef003910/cache-c70a3480c7835f24.arrow\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['genre', 'input_values', 'attention_mask'],\n",
       "        num_rows: 899\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['genre', 'input_values', 'attention_mask'],\n",
       "        num_rows: 100\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gtzan_encoded = gtzan.map(\n",
    "    preprocess_function,\n",
    "    remove_columns=[\"file\", \"audio\"],\n",
    "    batched=True,\n",
    "    num_proc=1,\n",
    ")\n",
    "\n",
    "gtzan_encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To enable the Trainer to process the class labels, we need to rename the genre column to label\n",
    "gtzan_encoded = gtzan_encoded.rename_column(\"genre\", \"label\")\n",
    "\n",
    "# to enable torch to process the class labels, we need to cast them to long"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we need to obtain the label mappings from the dataset. This mapping will take us from integer ids (e.g. 7) to human-readable class labels (e.g. \"pop\") and back again. In doing so, we can convert our modelâ€™s integer id prediction into human-readable format, enabling us to use the model in any downstream application. We can do this by using the int2str() method as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'pop'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id2label = {\n",
    "    str(i): id2label_fn(i)\n",
    "    for i in range(len(gtzan_encoded[\"train\"].features[\"label\"].names))\n",
    "}\n",
    "\n",
    "label2id = {v: k for k, v in id2label.items()}\n",
    "\n",
    "id2label[\"7\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "OK, weâ€™ve now got a dataset thatâ€™s ready for training! Letâ€™s take a look at how we can train a model on this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Fine-tuning the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of HubertForSequenceClassification were not initialized from the model checkpoint at ntu-spml/distilhubert and are newly initialized: ['projector.bias', 'classifier.weight', 'classifier.bias', 'projector.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Use the HF Trainer to fuine-tune the distilhubert model on the GTZAN dataset\n",
    "from transformers import AutoModelForAudioClassification\n",
    "\n",
    "num_labels = len(id2label)\n",
    "\n",
    "model = AutoModelForAudioClassification.from_pretrained(\n",
    "    model_id,\n",
    "    num_labels=num_labels,\n",
    "    label2id=label2id,\n",
    "    id2label=id2label,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We strongly advise you to upload model checkpoints directly the Hugging Face Hub while training. The Hub provides:\n",
    "\n",
    "* Integrated version control: you can be sure that no model checkpoint is lost during training.\n",
    "* Tensorboard logs: track important metrics over the course of training.\n",
    "* Model cards: document what a model does and its intended use cases.\n",
    "* Community: an easy way to share and collaborate with the community! ðŸ¤—\n",
    "\n",
    "Linking the notebook to the Hub is straightforward - it simply requires entering your Hub authentication token when prompted. Find your Hub authentication token here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a4ee0878c134446b9141180090f4ed0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next define the training arguments\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "model_name = model_id.split(\"/\")[-1]\n",
    "batch_size = 8\n",
    "gradient_accumulation_steps = 1\n",
    "num_train_epochs = 10\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    f\"{model_name}-finetuned-gtzan-1\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    warmup_ratio=0.1,\n",
    "    logging_steps=5,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"accuracy\",\n",
    "    fp16=True,\n",
    "    push_to_hub=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'distilhubert'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last thing we need to do is define the metrics. Since the dataset is balanced, weâ€™ll use accuracy as our metric and load it using the ðŸ¤— Evaluate library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"Computes accuracy on a batch of predictions\"\"\"\n",
    "    predictions = np.argmax(eval_pred.predictions, axis=1)\n",
    "    return metric.compute(predictions=predictions, references=eval_pred.label_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequence(feature=Value(dtype='int32', id=None), length=-1, id=None)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gtzan_encoded[\"train\"].features['attention_mask']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have all the pieces to train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/raj/repos/HF-Audio/4-music-genre-classifier/distilhubert-finetuned-gtzan-1 is already a clone of https://huggingface.co/RajkNakka/distilhubert-finetuned-gtzan-1. Make sure you pull the latest changes with `repo.git_pull()`.\n",
      "/home/raj/repos/HF-Audio/4-music-genre-classifier/4-envs/lib/python3.11/site-packages/transformers/optimization.py:407: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "/home/raj/repos/HF-Audio/4-music-genre-classifier/4-envs/lib/python3.11/site-packages/torch/nn/modules/conv.py:309: UserWarning: Applied workaround for CuDNN issue, install nvrtc.so (Triggered internally at /opt/conda/conda-bld/pytorch_1682343995622/work/aten/src/ATen/native/cudnn/Conv_v8.cpp:80.)\n",
      "  return F.conv1d(input, weight, bias, self.stride,\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1130' max='1130' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1130/1130 1:39:00, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1.753300</td>\n",
       "      <td>1.792695</td>\n",
       "      <td>0.470000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1.255500</td>\n",
       "      <td>1.279248</td>\n",
       "      <td>0.600000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.020900</td>\n",
       "      <td>1.027561</td>\n",
       "      <td>0.700000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.670300</td>\n",
       "      <td>0.818121</td>\n",
       "      <td>0.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.515200</td>\n",
       "      <td>0.739550</td>\n",
       "      <td>0.770000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.276300</td>\n",
       "      <td>0.649834</td>\n",
       "      <td>0.810000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.238600</td>\n",
       "      <td>0.677489</td>\n",
       "      <td>0.790000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.316200</td>\n",
       "      <td>0.629078</td>\n",
       "      <td>0.810000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.155000</td>\n",
       "      <td>0.612056</td>\n",
       "      <td>0.830000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.089400</td>\n",
       "      <td>0.666008</td>\n",
       "      <td>0.810000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1130, training_loss=0.7463425720687461, metrics={'train_runtime': 5950.2699, 'train_samples_per_second': 1.511, 'train_steps_per_second': 0.19, 'total_flos': 6.133988274624e+17, 'train_loss': 0.7463425720687461, 'epoch': 10.0})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# instantiate the trainer\n",
    "\n",
    "from transformers import Trainer\n",
    "\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    args=training_args,\n",
    "    train_dataset=gtzan_encoded[\"train\"],\n",
    "    eval_dataset=gtzan_encoded[\"test\"],\n",
    "    tokenizer=feature_extractor,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Submit my checkpoint to the leaderboard by pushing the training results to the Hub. We simply set the appropriate key word arguments (kwargs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs = {\n",
    "    \"dataset_tags\": \"marsyas/gtzan\",\n",
    "    \"dataset\": \"GTZAN\",\n",
    "    \"model_name\": f\"{model_name}-finetuned-gtzan-1\",\n",
    "    \"finetuned_from\": model_id,\n",
    "    \"tasks\": \"audio-classification\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upload the training results to the Hub using push_to_hub command"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "To https://huggingface.co/RajkNakka/distilhubert-finetuned-gtzan-1\n",
      "   a536ab6..77a7a27  main -> main\n",
      "\n",
      "To https://huggingface.co/RajkNakka/distilhubert-finetuned-gtzan-1\n",
      "   77a7a27..6819480  main -> main\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'https://huggingface.co/RajkNakka/distilhubert-finetuned-gtzan-1/commit/77a7a27d081d8a28b8871eb36b94a2e82f698ac6'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.push_to_hub(**kwargs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
